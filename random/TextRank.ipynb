{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {\n",
    "    'webpage-1': set(['webpage-2', 'webpage-4', 'webpage-5', 'webpage-6', 'webpage-8', 'webpage-9', 'webpage-10']),\n",
    "    'webpage-2': set(['webpage-5', 'webpage-6']),\n",
    "    'webpage-3': set(['webpage-10']),\n",
    "    'webpage-4': set(['webpage-9']),\n",
    "    'webpage-5': set(['webpage-2', 'webpage-4']),\n",
    "    'webpage-6': set([]), # dangling page\n",
    "    'webpage-7': set(['webpage-1', 'webpage-3', 'webpage-4']),\n",
    "    'webpage-8': set(['webpage-1']),\n",
    "    'webpage-9': set(['webpage-1', 'webpage-2', 'webpage-3', 'webpage-8', 'webpage-10']),\n",
    "    'webpage-10': set(['webpage-2', 'webpage-3', 'webpage-8', 'webpage-9']),\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'webpage-1': 0, 'webpage-2': 1, 'webpage-3': 2, 'webpage-4': 3, 'webpage-5': 4, 'webpage-6': 5, 'webpage-7': 6, 'webpage-8': 7, 'webpage-9': 8, 'webpage-10': 9}\n"
     ]
    }
   ],
   "source": [
    "def build_index(links):\n",
    "    website_list = links.keys()\n",
    "    return {website: index for index, website in enumerate(website_list)}\n",
    " \n",
    "website_index = build_index(links)\n",
    "print(website_index)\n",
    "# {'webpage-10': 3, 'webpage-9': 0, 'webpage-8': 1, 'webpage-1': 2, 'webpage-3': 4, 'webpage-2': 5, 'webpage-5': 6, 'webpage-4': 7, 'webpage-7': 8, 'webpage-6': 9}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.14285714 0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.14285714 0.14285714 0.14285714]\n",
      " [0.         0.         0.         0.         0.5        0.5\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.        ]\n",
      " [0.         0.5        0.         0.5        0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.1        0.1        0.1        0.1        0.1        0.1\n",
      "  0.1        0.1        0.1        0.1       ]\n",
      " [0.33333333 0.         0.33333333 0.33333333 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.2        0.2        0.2        0.         0.         0.\n",
      "  0.         0.2        0.         0.2       ]\n",
      " [0.         0.25       0.25       0.         0.         0.\n",
      "  0.         0.25       0.25       0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "def build_transition_matrix(links, index):\n",
    "    total_links = 0\n",
    "    A = np.zeros((len(index), len(index)))\n",
    "    for webpage in links:\n",
    "        # dangling page\n",
    "        if not links[webpage]:\n",
    "            # Assign equal probabilities to transition to all the other pages\n",
    "            A[index[webpage]] = np.ones(len(index)) / len(index)\n",
    "        else:\n",
    "            for dest_webpage in links[webpage]:\n",
    "                total_links += 1\n",
    "                A[index[webpage]][index[dest_webpage]] = 1.0 / len(links[webpage])\n",
    " \n",
    "    return A\n",
    " \n",
    "A = build_transition_matrix(links, website_index)\n",
    "print(A)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec1: [0.015 0.015 0.015 0.015 0.015 0.015 0.015 0.015 0.015 0.015]\n",
      "vec2: [0.16333333 0.11928571 0.08833333 0.10761905 0.07428571 0.07428571\n",
      " 0.01       0.06928571 0.14928571 0.14428571]\n",
      "vec4: [0.13883333 0.10139286 0.07508333 0.09147619 0.06314286 0.06314286\n",
      " 0.0085     0.05889286 0.12689286 0.12264286]\n",
      "vec3: [0.15383333 0.11639286 0.09008333 0.10647619 0.07814286 0.07814286\n",
      " 0.0235     0.07389286 0.14189286 0.13764286]\n",
      "new_P: [0.15383333 0.11639286 0.09008333 0.10647619 0.07814286 0.07814286\n",
      " 0.0235     0.07389286 0.14189286 0.13764286]\n",
      "delta: 0.3124761904761905\n",
      "\n",
      "\n",
      "vec1: [0.015 0.015 0.015 0.015 0.015 0.015 0.015 0.015 0.015 0.015]\n",
      "vec2: [0.11791905 0.13165119 0.0784369  0.07669524 0.0879869  0.0879869\n",
      " 0.00781429 0.09257976 0.17067738 0.14825238]\n",
      "vec4: [0.10023119 0.11190351 0.06667137 0.06519095 0.07478887 0.07478887\n",
      " 0.00664214 0.0786928  0.14507577 0.12601452]\n",
      "vec3: [0.11523119 0.12690351 0.08167137 0.08019095 0.08978887 0.08978887\n",
      " 0.02164214 0.0936928  0.16007577 0.14101452]\n",
      "new_P: [0.11523119 0.12690351 0.08167137 0.08019095 0.08978887 0.08978887\n",
      " 0.02164214 0.0936928  0.16007577 0.14101452]\n",
      "delta: 0.15031440476190486\n",
      "\n",
      "\n",
      "Results: None\n"
     ]
    }
   ],
   "source": [
    "def pagerank(A, eps=0.0001, d=0.85):\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    count = 0\n",
    "    while True:\n",
    "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
    "        \n",
    "        vec1 = np.ones(len(A)) * (1 - d) / len(A)\n",
    "        print('vec1:',vec1)\n",
    "        \n",
    "        vec2 = A.T.dot(P)\n",
    "        print('vec2:',vec2)\n",
    "        \n",
    "        vec4 = d * vec2\n",
    "        print('vec4:',vec4)\n",
    "        \n",
    "        vec3 = vec1 + vec4\n",
    "        print('vec3:',vec3)\n",
    "        print('new_P:',new_P)\n",
    "        \n",
    "        delta = abs(new_P - P).sum()\n",
    "        print('delta:',delta)\n",
    "        print('\\n')\n",
    "        if count == 1 :\n",
    "            break\n",
    "        count = count+1\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        P = new_P\n",
    " \n",
    "results = pagerank(A)\n",
    " \n",
    "print(\"Results:\", results) # [ 0.13933698,  0.09044235,  0.1300934 ,  0.13148714,  0.08116465, 0.1305122 ,  0.09427366,  0.085402  ,  0.02301397,  0.09427366]\n",
    "# print(sum(results)) # 1.0\n",
    "# print([item[0] for item in sorted(enumerate(results), key=lambda item: -item[1])]) # [0, 3, 5, 2, 6, 9, 1, 7, 4, 8]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999999999999998\n",
      "0.4999999999999999\n",
      "0.9999999999999998\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    " \n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "#     print('vector1: ', vector1)\n",
    "#     print('vector2: ', vector2)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "#     print('vector1: ', vector1)\n",
    "#     print('vector2: ', vector2)\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "# One out of 5 words differ => 0.8 similarity\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split()))\n",
    " \n",
    "# One out of 2 non-stop words differ => 0.5 similarity\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split(), stopwords.words('english')))\n",
    " \n",
    "# 0 out of 2 non-stop words differ => 1 similarity (identical sentences)\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a good sentence\".split(), stopwords.words('english')))\n",
    " \n",
    "# Completely different sentences=> 0.0\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"I want to go to the market\".split(), stopwords.words('english')))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContent() :\n",
    "    filepath = '/Users/seeni-2328/Documents/Seeni/Datasets/zoho/connect/sridhar_post.txt'\n",
    "    F = open(filepath,'r') \n",
    "    content = []\n",
    "    for i in F :\n",
    "        for j in i.split('. ') :\n",
    "            j = j.strip().lower()\n",
    "            if len(j) > 0 :\n",
    "                content.append(j.split())\n",
    "#                 print(j)\n",
    "#     print(content)\n",
    "    return content\n",
    "# sentence = getContent()\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "[0.         0.07413297 0.         0.05222536 0.06187012 0.07937287\n",
      " 0.07031055 0.04710984 0.08347556 0.01670336 0.04043617 0.05653181\n",
      " 0.00570528 0.00746997 0.         0.00570528 0.08068491 0.05653181\n",
      " 0.06249826 0.         0.03768787 0.04607433 0.05222536 0.\n",
      " 0.04762372 0.01562457]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    " \n",
    "# Get a text from the Brown Corpus\n",
    "# sentences = brown.sents('ca01')\n",
    "sentences = getContent()\n",
    " \n",
    "# print(sentences)\n",
    "# [[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.'], [u'The', u'jury', u'further', u'said', u'in', u'term-end', u'presentments', u'that', u'the', u'City', u'Executive', u'Committee', u',', u'which', u'had', u'over-all', u'charge', u'of', u'the', u'election', u',', u'``', u'deserves', u'the', u'praise', u'and', u'thanks', u'of', u'the', u'City', u'of', u'Atlanta', u\"''\", u'for', u'the', u'manner', u'in', u'which', u'the', u'election', u'was', u'conducted', u'.'], ...]\n",
    " \n",
    "print(len(sentences))  #  98\n",
    " \n",
    "# get the english list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    " \n",
    " \n",
    "def build_similarity_matrix(sentences, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "#             S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S\n",
    " \n",
    "S = build_similarity_matrix(sentences, stop_words)    \n",
    "print(S[0])\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec1: [0.00576923 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923\n",
      " 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923\n",
      " 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923\n",
      " 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923\n",
      " 0.00576923 0.00576923]\n",
      "vec2: [0.04535467 0.05334193 0.00744825 0.0494695  0.05408504 0.05140178\n",
      " 0.04399183 0.03618195 0.05678306 0.01515823 0.05112926 0.04762827\n",
      " 0.02409259 0.06446579 0.01671087 0.02858216 0.05173996 0.04936679\n",
      " 0.04045405 0.01591658 0.03089873 0.043672   0.04193135 0.00151732\n",
      " 0.06919835 0.00947968]\n",
      "vec4: [0.03855147 0.04534064 0.00633101 0.04204907 0.04597229 0.04369152\n",
      " 0.03739305 0.03075466 0.0482656  0.0128845  0.04345987 0.04048403\n",
      " 0.0204787  0.05479592 0.01420424 0.02429484 0.04397896 0.04196177\n",
      " 0.03438594 0.0135291  0.02626392 0.0371212  0.03564165 0.00128972\n",
      " 0.0588186  0.00805773]\n",
      "vec3: [0.0443207  0.05110987 0.01210024 0.0478183  0.05174152 0.04946075\n",
      " 0.04316228 0.03652389 0.05403483 0.01865373 0.0492291  0.04625326\n",
      " 0.02624793 0.06056515 0.01997347 0.03006407 0.04974819 0.047731\n",
      " 0.04015517 0.01929833 0.03203315 0.04289043 0.04141088 0.00705895\n",
      " 0.06458783 0.01382696]\n",
      "new_P: [0.0443207  0.05110987 0.01210024 0.0478183  0.05174152 0.04946075\n",
      " 0.04316228 0.03652389 0.05403483 0.01865373 0.0492291  0.04625326\n",
      " 0.02624793 0.06056515 0.01997347 0.03006407 0.04974819 0.047731\n",
      " 0.04015517 0.01929833 0.03203315 0.04289043 0.04141088 0.00705895\n",
      " 0.06458783 0.01382696]\n",
      "delta: 0.3376693249731355\n",
      "\n",
      "\n",
      "vec1: [0.00576923 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923\n",
      " 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923\n",
      " 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923\n",
      " 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923 0.00576923\n",
      " 0.00576923 0.00576923]\n",
      "vec2: [0.04924586 0.05416162 0.00838545 0.05022325 0.05478526 0.05767515\n",
      " 0.0508971  0.03748554 0.05920687 0.01922681 0.0505001  0.0552291\n",
      " 0.02382588 0.0281187  0.01702281 0.01880991 0.05646482 0.05315368\n",
      " 0.04600356 0.01506301 0.03623362 0.04636589 0.04208874 0.00238932\n",
      " 0.05669359 0.01074438]\n",
      "vec4: [0.04185898 0.04603738 0.00712763 0.04268976 0.04656747 0.04902388\n",
      " 0.04326253 0.03186271 0.05032584 0.01634279 0.04292508 0.04694474\n",
      " 0.020252   0.0239009  0.01446939 0.01598842 0.0479951  0.04518063\n",
      " 0.03910302 0.01280356 0.03079857 0.039411   0.03577543 0.00203092\n",
      " 0.04818955 0.00913272]\n",
      "vec3: [0.04762821 0.05180661 0.01289687 0.04845899 0.0523367  0.05479311\n",
      " 0.04903176 0.03763194 0.05609507 0.02211202 0.04869431 0.05271397\n",
      " 0.02602123 0.02967013 0.02023862 0.02175765 0.05376433 0.05094986\n",
      " 0.04487226 0.01857279 0.03656781 0.04518023 0.04154466 0.00780015\n",
      " 0.05395878 0.01490196]\n",
      "new_P: [0.04762821 0.05180661 0.01289687 0.04845899 0.0523367  0.05479311\n",
      " 0.04903176 0.03763194 0.05609507 0.02211202 0.04869431 0.05271397\n",
      " 0.02602123 0.02967013 0.02023862 0.02175765 0.05376433 0.05094986\n",
      " 0.04487226 0.01857279 0.03656781 0.04518023 0.04154466 0.00780015\n",
      " 0.05395878 0.01490196]\n",
      "delta: 0.10263503381345297\n",
      "\n",
      "\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-9347e88b8f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get the sentences ordered by rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mranked_sentence_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_ranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_sentence_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter \n",
    " \n",
    "sentence_ranks = pagerank(S)\n",
    " \n",
    "print(sentence_ranks)\n",
    " \n",
    "# Get the sentences ordered by rank\n",
    "ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "print(ranked_sentence_indexes)\n",
    " \n",
    "# Suppose we want the 5 most import sentences\n",
    "SUMMARY_SIZE = 5\n",
    "SELECTED_SENTENCES = sorted(ranked_sentence_indexes[:SUMMARY_SIZE])\n",
    "print(SELECTED_SENTENCES)\n",
    " \n",
    "# Fetch the most important sentences\n",
    "summary = itemgetter(*SELECTED_SENTENCES)(sentences)\n",
    " \n",
    "# Print the actual summary\n",
    "for sentence in summary:\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "2. Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said .\n",
      "3. -- After a long , hot controversy , Miller County has a new school superintendent , elected , as a policeman put it , in the `` coolest election I ever saw in this county '' .\n",
      "4. `` This was the coolest , calmest election I ever saw '' , Colquitt Policeman Tom Williams said .\n",
      "5. `` Everything went real smooth '' , the sheriff said .\n"
     ]
    }
   ],
   "source": [
    "def textrank(sentences, top_n=5, stopwords=None):\n",
    "    \"\"\"\n",
    "    sentences = a list of sentences [[w11, w12, ...], [w21, w22, ...], ...]\n",
    "    top_n = how may sentences the summary should contain\n",
    "    stopwords = a list of stopwords\n",
    "    \"\"\"\n",
    "    S = build_similarity_matrix(sentences, stop_words) \n",
    "    sentence_ranks = pagerank(S)\n",
    " \n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(sentences)\n",
    "    return summary\n",
    " \n",
    "for idx, sentence in enumerate(textrank(sentences, stopwords=stopwords.words('english'))):\n",
    "    print(\"%s. %s\" % ((idx + 1), ' '.join(sentence)))\n",
    " \n",
    "# 1. `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
    "# 2. Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said .\n",
    "# 3. -- After a long , hot controversy , Miller County has a new school superintendent , elected , as a policeman put it , in the `` coolest election I ever saw in this county '' .\n",
    "# 4. `` This was the coolest , calmest election I ever saw '' , Colquitt Policeman Tom Williams said .\n",
    "# 5. `` Everything went real smooth '' , the sheriff said .\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
